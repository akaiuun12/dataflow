---
title: Ensemble (Bagging, Boosting, Stacking)
date: 2026-01-11
author: Akai Red
image: imgs/2026-01-11-ensemble.png
tags: ['ml', 'ensemble']
published: true
---

단일 모델의 성능이 기대에 미치지 못할 때, 여러 개의 모델을 결합하여 더 나은 성능을 내는 기법을 **앙상블 학습(Ensemble Learning)**이라고 한다. "집단 지성"처럼 여러 모델의 지혜를 모아 오차를 줄이고 일반화 성능을 높이는 것이 핵심이다.


## Ensemble Learning

앙상블 학습은 다양한 방식이 존재하는데, 보팅, 배깅, 부스팅이 대표적이다.

* **보팅(Voting)**: 각기 다른 모델을 학습시킨 뒤, 예측 결과를 평균내어 최종 예측을 결정하는 방식이다.
* **배깅(Bagging)**: 데이터를 랜덤으로 추출하여 **병렬적**으로 여러 개의 모델을 학습시키는 방식이다.
* **부스팅(Boosting)**: 이전 모델의 예측 결과를 보완하여 **순차적**으로 새로운 모델을 학습시키는 방식이다.
* **스태킹(Stacking)**: 여러 개의 모델을 학습시킨 뒤, 예측 결과를 취합하는 **메타 모델 또한 학습**시키는 방식이다.


## Voting Classifier

**각기 다른 훈련 방식을 가진 분류기**들을 결합하여 최종 예측 결과를 결정하는 방식이다. 보팅 방식에는 크게 두 가지가 있다.

### 하드 보팅 (Hard Voting)
다수결의 원칙과 같다. 각 분류기가 예측한 결과값 중 가장 많이 나온 클래스를 최종 결과로 선택한다.

### 소프트 보팅 (Soft Voting)
각 분류기가 예측한 클래스별 확률을 평균내어 가장 확률이 높은 클래스를 선택한다. 확률이 높은 투표에 비중을 더 두기 떄문에, 일반적으로 하드 보팅보다 성능이 좋다.


## Bagging & Pasting

배깅은 같은 알고리즘을 사용하고, **훈련 세트의 서브셋을 랜덤으로 추출하여 병렬적으로 훈련**시키는 방식이다. 의사결정트리(DT)를 사용한 랜덤 포레스트(Random Forest)가 가장 대표적인 배깅 알고리즘이다. 랜덤포레스트는 파라미터 설정이 간편하고 학습속도가 빠르면서도 안정적인 성능을 내기 때문에 많은 ML 문제의 베이스라인 모델로 사용한다.

* 배깅(Bagging): 중복을 허용하여 추출
* 패이스팅(Pasting): 중복을 허용하지 않고 추출

일반적으로는 페이스팅보다는 배깅을 사용한다. 배깅 및 페이스팅은 각 예측기를 독립적으로 학습시킨다. 따라서 병렬처리가 가능하고 확장이 용이하다는 장점이 있다. 최종적으로 학습시킨 각 예측기의 예측 결과를 보팅을 통해 결정하거나(분류), 평균을 내어 최종 예측을 결정(회귀)한다.

![Bagging은 중복을 허용하고 Pasting은 중복을 허용하지 않고 추출한다.](imgs/2026-01-11-ensemble.png)  


## Boosting

부스팅은 **약한 학습기를 순차적으로 연결**하여 강한 학습기를 만드는 방식이다. AdaBoost, Gradient Boosting 등이 대표적인 예이다.

### Adaptive Boosting (AdaBoost)

Adaptive Boosting(AdaBoost)는 이전 예측기를 보완하는 방식으로, 이전 예측기가 틀린 데이터에 가중치를 부여하여 다음 예측기를 학습시킨다. 이럴 경우, 이전 예측기가 틀린 데이터에 더 집중하여 학습하게 되어 새로운 예측기는 학습하기 어려운 데이터에 더 잘 학습하게 된다.

![AdaBoost의 학습 과정: 이전 단계의 오차에 가중치를 부여하여 순차적으로 학습한다.](imgs/2026-01-11-ensemble-01.png)  

모든 예측기가 훈련을 마치면 최종 예측은 모든 예측기의 결과를 평균내어 결정한다. 이 때 각 예측기의 전반적인 정확도에 따라 예측기마다 다른 가중치를 부여한다. 보다 정확도가 높은 예측기의 가중치가 더 높아 최종 예측에 더 큰 영향을 미친다.

AdaBoost는 초창기 부스팅 모델로 이상치에 매우 민감하다. 특정 데이터가 이상치라서 계속 틀리게 되면, 그 데이터의 가중치가 비정상적으로 커져서 모델 전체가 망가질 수 있기 때문이다. 그래서 현대에는 이상치에 덜 민감한 Gradient Boosting이 더 많이 사용된다.


### Gradient Boosting

Gradient Boosting도 AdaBoost와 마찬가지로 이전 예측기를 보완하여 새로운 예측기를 연결하는 방식이다. 다만 AdaBoost와 다르게 샘플의 가중치를 수정하는 대신 이전 예측기가 만든 잔차(Residual Error)를 타겟으로 하여 다음 예측기를 학습시킨다.

![Gradient Boosting: 이전 예측기의 잔차를 학습하여 오차를 줄여나간다.](imgs/2026-01-11-ensemble-02.png)  

첫번째 열을 보면 트리가 하나만 있어, 앙상블의 값이 트리와 동일하다. 두번째 행은 첫번째 트리의 잔차를 학습했다. 앙상블 모델이 보다 더 정확한 예측을 할 수 있게 되었다. 트리가 앙상블에 추가될 수록 예측 오차가 줄어들고 정확도가 높아지는 것을 확인할 수 있다.


## Stacking

스태킹(Stacking)은 '앙상블에 속한 모든 예측기의 예측을 단순히 평균내어 최종 예측을 결정하는 것이 아니라, 취합하는 모델을 훈련시킬 수는 없을까?'라는 질문에서 시작되었다. 

가장 간단한 2-Stage Stacking을 예로 들어보자. (사실 2-Stage Stacking으로도 대부분의 경우 충분하다.)

- **1. 첫번째 단계**: 여러 개의 예측기를 학습시킨 후, 원본 데이터에 대한 OOF 예측치를 생성한다.
- **2. 두번째 단계**: 첫번째 단계의 OOF 예측치를 타겟으로 하여 메타 모델을 학습시킨다.

첫번째 단계의 각 모형은 서로 상관관계가 낮은 모형을 사용하는 것이 좋다. XGBoost, LightGBM 등의 부스팅 기반 모형과 신경망 모형을 조합할 수도 있다. 첫번째 단계의 각 모형들이 높은 다중공선성을 지닐 수 있으므로, 두번째 단계의 메타 모델로는 보다 단순하며 다중공선성에 강한 Ridge Regression을 주로 사용한다.

`scikit-learn`의 `StackingClassifier`와 `StackingRegressor`를 사용하여 간단하게 구현할 수 있지만.. 직접 컨트롤해야할 일이 많기 때문에 `kFold` 등을 사용하여 Stacking 모델을 직접 구현하는 것이 더 좋다.