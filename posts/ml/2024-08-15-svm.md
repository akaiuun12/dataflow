---
layout: post
category: ML
tags: ['data science']
date: 2024-08-15

title: Support Vector Machine
author: Akai Red
description:
    데이터를 구분하는 최적의 결정 경계를 만드는 것이 SVM의 목적이다. 
image: 
optimized_image: 

show_thumbnail: true
math: true
published: true
---


## 서포트벡터 머신 Support Vector Machine

서포트벡터 머신은 기존 데이터를 바탕으로 새로운 데이터를 분류하는 지도 학습 방법이다. 이진분류*Binary Classification*에서 주로 사용하지만 다중분류나 회귀에서도 사용할 수 있다. 


### 초평면 Hyperplane, 마진 Margin

서로 다른 두 클래스를 구분하는 직선(혹은 평면)을 **초평면*Hyperplane***이라고 한다. SVM의 목표는 두 클래스를 구분하는 여러 직선 중 최선의 직선을 구하는 것이다. '최선'의 기준으로 사용하는 것이 바로 **마진*Margin***인데 초평면과 최근접 데이터 사이의 거리를 의미한다. 추가로 초평면과 가장 근접한 데이터를 **서포트벡터*Support Vector***라고 부른다.

![picture 4](/imgs/315a40930e4bee40b001f4410851e6120d4c3bb160f6840a38b11c1fa2044b0c.png)  


## 소프트마진 분류 Soft Margin Classification

모든 데이터를 초평면으로 정확하게 분류하는 것은 사실상 불가능하다. 어느 정도의 분류 오차를 허용하여, 이상치의 영향을 줄이고 과적합을 방지하는 방법이 소프트마진 분류다. 수학적으로는 슬랙 변수 $\xi$라는 것을 도입해서 오차를 허용하는데 자세한 수학적 내용은 생략한다. 일반적으로 하이퍼파라미터 C를 사용해서 SVM의 오차 허용도를 조절한다. C가 커질수록 오차 허용도가 커지고 모형은 유연해진다.

![picture 5](/imgs/cff5f66defd73700053aedfdc9224cfe91be39ac60c93a327e344f92eae64e8b.png)  


## SVM with scikit-learn
```python
# !pip install ucimlrepo
from ucimlrepo import fetch_ucirepo 
  
# 1. fetch dataset 
ilpd_indian_liver_patient_dataset = fetch_ucirepo(id=225) 
  
# 2. data (as pandas dataframes) 
X = ilpd_indian_liver_patient_dataset.data.features 
y = ilpd_indian_liver_patient_dataset.data.targets

# 3. categorization
X['Gender'] = X['Gender'].astype('category').cat.codes
y = (y == 1)

# 4. fill null values
X['A/G Ratio'].fillna(X['A/G Ratio'].mean(), inplace=True)

# 5. train-test split
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                    stratify=y, test_size=0.3, random_state=42)
print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)

# 6. scaling
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
```

SVM 실습을 위해 [ILPD (Indian Liver Patient Dataset)](https://archive.ics.uci.edu/dataset/225/ilpd+indian+liver+patient+dataset)를 사용했다. 10개의 변수로 간 질환 발생 여부를 예측하는 것이 목적이다. 데이터에 대한 보다 자세한 설명은 링크에서 확인할 수 있다. 

데이터를 불러온 뒤 `Gender`변수의 데이터 타입을 적절하게 변환하였으며, 전체 데이터에 대해 스케일링을 해주었다. SVM은 데이터 스케일에 민감하기 때문에 스케일링을 해줄 필요가 있다. 또한 `A/G Ratio`의 NULL 값을 평균치로 대체하였다.

```python
# SVM Model Specification
from sklearn.svm import SVC, LinearSVC # 선형 SVM일 경우 LinearSVC가 SVC보다 빠르다.
from sklearn.linear_model import SGDClassifier 

svc = SVC(kernel='linear', C=1, random_state=42)  # C: 규제 파라미터
lsvc = LinearSVC(C=1, dual='auto', random_state=42)
sgd_hinge = SGDClassifier(loss='hinge', penalty='l2', random_state=42)  # loss='hinge': SVM

svc.fit(X_train, y_train.values.ravel())
lsvc.fit(X_train, y_train.values.ravel())
sgd_hinge.fit(X_train, y_train.values.ravel())

# Model Validation
from sklearn.metrics import accuracy_score

y_pred_svc = svc.predict(X_test)
y_pred_lsvc = lsvc.predict(X_test)
y_pred_sgd_hinge = sgd_hinge.predict(X_test)

print(f'SVC Accuracy: {accuracy_score(y_test, y_pred_svc):.2f}')
print(f'LSVC Accuracy: {accuracy_score(y_test, y_pred_lsvc):.2f}')
print(f'SGD Classifier Accuracy: {accuracy_score(y_test, y_pred_sgd_hinge):.2f}')
```

기본적으로 `sklearn.svm.SVC`를 사용하면 SVM를 수행할 수 있다. 하이퍼파라미터는 `C`로 위에서 설명한대로 오차 허용도를 나타낸다. 메모리 및 속도 성능 개선을 위해 `LinearSVC`나 `SGDClassifier`를 사용하기도 한다. 선형 분류에서는 일반적으로 `LinearSVC`가 `SVC`보다 빠르고 정확하게 분류하는 것으로 알려져 있다.


<!-- ## 비선형 분류를 위한 kernel SVM
추후 작성 예정

## 서포트벡터 회귀 Support Vector Regression
추후 작성 예정 -->

## References
- [Support Vector Machine - MATLAB & Simulink](https://kr.mathworks.com/discovery/support-vector-machine.html)
- [SVM using Scikit-Learn in Python | LearnOpenCV](https://learnopencv.com/svm-using-scikit-learn-in-python/)
- [ILPD (Indian Liver Patient Dataset)](https://archive.ics.uci.edu/dataset/225/ilpd+indian+liver+patient+dataset)