---
title: 5-Fold Cross-Validation
date: 2026-01-04
author: Akai Red

image: https://scikit-learn.org/stable/_images/grid_search_cross_validation.png
tags: ['machine learning', 'cross-validation', 'model evaluation']

published: true
---

**교차 검증*Cross-Validation***은 데이터를 여러 번 나누어 모델을 학습시키고 평가하는 기법으로, 특정 데이터셋에 모델이 과적합*Overfitting*되는 것을 방지하고 모델의 일반화 성능을 객관적으로 측정하기 위해 사용한다. 그 중에서도 **5-겹 교차 검증 *5-Fold Cross-Validation***은 가장 널리 쓰이는 형태 중 하나다.

## 5-Fold Cross-Validation

```python
from sklearn.model_selection import KFold, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import load_iris

# 데이터 로드
iris = load_iris()
X, y = iris.data, iris.target

# 모델 정의
model = RandomForestClassifier(random_state=42)

# 5-Fold Cross-Validation 설정
kf = KFold(n_splits=5, shuffle=True, random_state=42)

print(f"각 폴드의 정확도: {scores}")
print(f"평균 정확도: {scores.mean():.4f}")
```

모델을 단순히 Train/Test set으로 한 번만 나누어 평가할 경우, 우연히 모델이 평가하기 쉬운 데이터가 Test set에 포함되거나 그 반대의 경우가 발생할 수 있다. 교차 검증은 이러한 '운'의 요소를 최소화하고 전체 데이터에 대한 모델의 견고함을 확인하게 해준다. 

> **왜 하필 5-Fold일까?** 사실 K값에 정답은 없지만, 일반적으로 5 또는 10이 가장 많이 쓰인다. 

## How it Works

5-Fold Cross-Validation은 다음과 같은 단계로 진행된다:

1.  **데이터 분할**: 전체 데이터셋을 동일한 크기를 가진 5개의 그룹(Fold)으로 나눈다.
2.  **반복 학습 및 평가**:
    - 1번째 폴드를 Validation set, 나머지 4개를 Train set으로 사용하여 학습시킨다.
    - 학습된 모델을 1번째 폴드로 평가하여 점수를 기록한다.
    - 이 과정을 각 폴드가 한 번씩 Validation set이 되도록 총 5번 반복한다.
3.  **결과 산출**: 5번의 평가 점수를 평균 내어 최종 모델의 성능으로 간주한다.

![5-fold cross validation diagram](https://scikit-learn.org/stable/_images/grid_search_cross_validation.png)
*(이미지 출처: scikit-learn official documentation)*

## Implementation with Scikit-Learn

`scikit-learn` 라이브러리를 사용하면 교차 검증을 매우 간단하게 구현할 수 있다.

### 1. cross_val_score
가장 간단한 방법으로, 모델의 성능(점수)만 빠르게 확인하고 싶을 때 사용한다. 하지만 학습된 모델을 저장하지 않고 점수만 계산한 뒤 버리기 때문에, 나중에 설명할 **OOF 예측값이나 테스트 데이터에 대한 예측값(Inference)을 얻을 수 없다**는 한계가 있다.

```python
from sklearn.model_selection import cross_val_score

# cv 파라미터에 폴드 수나 KFold 객체를 전달
scores = cross_val_score(model, X, y, cv=5)
print(f"Scores: {scores}")
```

### 2. KFold
직접 분할을 하고 싶을 때는 `KFold` 클래스를 사용한다. 특정 순서로 데이터가 수집되었을 경우 `shuffle=True` 옵션을 사용해서 데이터를 무작위로 섞어서 분할해야 한다. 이렇게 하지 않으면 모델이 학습 과정에서 특정 순서로 데이터가 수집되었다는 정보를 알게 되어 과적합을 유발할 수 있다.

```python
from sklearn.model_selection import KFold

kf = KFold(n_splits=5, shuffle=True, random_state=42)

for train_idx_, val_idx_ in kf.split(X):
    X_train, X_val = X[train_idx_], X[val_idx_]
    y_train, y_val = y[train_idx_], y[val_idx_]
    
    # 이후 학습 및 평가 로직 진행
    # ...
```

### 3. Stratified K-Fold

**분류 문제에서 타겟 라벨의 분포가 불균형할 경우**, 일반적인 `KFold`를 쓰면 특정 폴드에 특정 클래스가 아예 포함되지 않는 문제가 생길 수 있다. 이때는 **층화 K-겹 교차 검증*Stratified K-Fold***를 사용하여 각 폴드 내의 클래스 비율을 전체 데이터의 비율과 동일하게 유지하는 것이 필수적이다. 회귀 문제에서는 사용하지 않는다.

```python
from sklearn.model_selection import StratifiedKFold

skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

for train_idx_, val_idx_ in skf.split(X, y):
    X_train, X_val = X[train_idx_], X[val_idx_]
    y_train, y_val = y[train_idx_], y[val_idx_]
    
    # 이후 학습 및 평가 로직 진행
    # ...
```

## OOF (Out-Of-Fold)

교차 검증을 수행하고 각 폴드의 검증 데이터에 대한 예측값을 차례대로 쌓으면, **전체 학습 데이터와 동일한 크기의 예측치**를 얻을 수 있다. 이를 OOF 예측값이라고 한다. 학습 과정에서 검증 데이터(Validation set)는 모델 입장에서 한 번도 보지 못한 데이터다. OOF 예측값을 만들면 모든 학습 데이터를 한 번씩은 검증에 사용하게 되므로 데이터 낭비를 최소화하고 활용도를 극대화할 수 있다.

```python
import numpy as np
from sklearn.metrics import accuracy_score

# 1. OOF 예측값과 테스트 데이터 예측값을 저장할 배열 초기화
oof_preds = np.zeros(len(X))
test_preds = np.zeros(len(X_test)) # 예측해야 할 테스트 데이터

kf = KFold(n_splits=5, shuffle=True, random_state=42)

for i, (train_idx, val_idx) in enumerate(kf.split(X)):
    X_train, X_val = X[train_idx], X[val_idx]
    y_train, y_val = y[train_idx], y[val_idx]
    
    # 모델 학습
    model = RandomForestClassifier(random_state=42)
    model.fit(X_train, y_train)
    
    # 2. 검증 데이터에 대한 OOF 예측 저장
    val_preds = model.predict(X_val)
    oof_preds[val_idx] = val_preds
    
    # 3. 테스트 데이터에 대한 예측 (5개 모델의 결과를 평균냄)
    test_preds += model.predict(X_test) / 5
    
    print(f"Fold {i+1} Accuracy: {accuracy_score(y_val, val_preds):.4f}")

# 최종 OOF 성능 확인
print(f"Overall OOF Accuracy: {accuracy_score(y, oof_preds):.4f}")
```

`oof_preds`는 전체 학습 데이터에 대한 모형의 예측값이 저장되어 있다. `test_preds`는 테스트 데이터에 대한 모형의 예측값이 저장되어 있다. 5개의 모델을 학습시키고 각 모델의 결과를 평균 내어 최종 예측값을 만드는 전체 흐름이다.

`oof_pred`는 학습 데이터에 대한 Fresh한 예측값이라고 할 수 있겠다. 일반적으로 모형 학습에 사용한 데이터를 다시 평가에 사용하는 것은 좋지 않다. 그러나 OOF 예측값은 보지 못한 데이터에 대한 예측값이므로 문제가 없다.

`oof_pred`와 `y_val`의 값을 비교하면 모형의 Cross-Validation 성능을 평가할 수 있다. 마지막 print문에서 확인하는 `accuracy_score`는 모형의 Cross-Validation 성능을 평가하는 지표로, 위에서 `cross_val_score`를 사용했을 때와 동일한 값을 보인다.

> **Tip: cross_val_predict와의 차이** Scikit-learn의 `cross_val_predict` 함수를 쓰면 OOF 예측값을 한 줄로 얻을 수 있다. 하지만 이 함수 역시 테스트 데이터(`X_test`)에 대한 예측은 수행할 수 없으므로, Kaggle 등의 경진대회에서는 주로 위와 같이 `for` 루프를 직접 사용하여 OOF와 테스트 예측을 동시에 처리하는 방식을 선호한다.


## References
* [Scikit-learn Cross-validation Guide](https://scikit-learn.org/stable/modules/cross_validation.html)
* [Cross-validation (statistics) - Wikipedia](https://en.wikipedia.org/wiki/Cross-validation_(statistics))
