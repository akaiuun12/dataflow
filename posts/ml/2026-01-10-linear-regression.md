---
title: Linear, Lasso, Ridge Regression
date: 2026-01-10
author: Akai Red

image: /imgs/2026-01-10-linear-lasso-ridge-regression-01.png
tags: ['machine learning', 'regression', 'regularization', 'linear regression', 'ridge', 'lasso']

published: false
---

회귀 분석은 데이터를 바탕으로 연속형 타겟을 예측한다. 데이터의 선형성이 강할 경우, 선형 모형에 적합하는 선형 회귀 분석을 수행하게 된다. 본 글에서는 선형 회귀(Linear Regression)와, 과적합을 방지하기 위해 도입된 규제(Regularization) 기법인 릿지(Ridge) 및 라쏘(Lasso) 회귀에 대해 살펴본다.


## 1. 선형 회귀 (Linear Regression)
선형 회귀는 종속 변수 $y$와 하나 이상의 독립 변수 $x$ 사이의 선형 상관관계를 모델링하는 방법이다. 조금 더 쉽게 말하면, 데이터와 가장 근접한 직선(혹은 초평면)을 찾는 것이다.

![선형 회귀의 원리: 잔차 제곱합 최소화](imgs/2026-01-10-linear-lasso-ridge-regression-02.png)  

'가장 근접하다'는 것은 그렇게 수학적인 표현이 아니다. 이를 수학적으로 표현하면, 잔차 제곱합(RSS: Residual Sum of Squares)을 최소화하는 회귀 계수의 조합을 찾는 것이 된다. 잔차 제곱합이 우리가 최소화하고자 하는 목적 함수가 된다.

### 목적 함수
위에서 말했듯이 선형 회귀의 목표는 실제값과 모델의 예측값 사이의 차이인 잔차 제곱합을 최소화하는 회귀 계수 $w$를 찾는 것이다. 잔차 제곱합을 수학적으로 표현하면 다음과 같다. 

$$
RSS(w) = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 = \sum_{i=1}^{n} (y_i - \sum_{j=0}^{p} w_j x_{ij})^2
$$

이 수식은 위의 그림에서 회귀 직선과 실제 데이터 사이의 거리(의 제곱)을 모두 더한 것과 동일하다. 데이터와 모형의 차이(잔차)를 제곱하여, 모두 더해서 잔차 제곱합이다. 이 얼마나 단순한 작명인가. 


### 한계점
선형 회귀는 단순하면서도 강력한 모형이지만 다음과 같은 한계점이 있다.

* **과적합(Overfitting)**
: 데이터의 노이즈까지 학습하여 새로운 데이터에 대한 일반화 성능이 떨어질 수 있다. 특히 독립 변수가 많을수록 모델이 복잡해져 과적합 가능성이 높아진다.

* **다중공선성(Multicollinearity)**
: 독립 변수들 간의 상관관계가 높을 때 모델이 불안정해진다.

위 문제는 선형 회귀만의 문제는 아니다. 비선형 모형에서도 발생할 수 있는 문제이다. 선형 회귀에는 이러한 문제를 해결하기 위해 **규제(Regularization)**를 도입한 릿지 회귀, 라쏘 회귀가 있다.


## 2. 규제 (Regularization)
규제는 모델의 복잡도를 줄이기 위해 가중치(Weights)에 페널티를 부여하는 방법이다. 규제를 통해 모델이 훈련 데이터에 너무 민감하게 반응하지 않도록 조절한다.

주로 사용되는 규제 기법은 가중치의 제곱을 이용하는 **L2 규제(Ridge)**와 가중치의 절대값을 이용하는 **L1 규제(Lasso)**가 있다.

![릿지 회귀와 라쏘 회귀의 규제 영역](imgs/2026-01-10-linear-lasso-ridge-regression-01.png)  


## 3. 릿지 회귀 (Ridge Regression) - L2 규제
릿지 회귀는 기존 선형 회귀의 RSS에 **가중치들의 제곱합**에 비례하는 페널티 항을 추가한 것이다. 릿지 회귀를 사용하면 과적합을 방지하고, 다중공선성 문제를 해결할 수 있다.

### 목적 함수
$$
J(w) = RSS(w) + \alpha \sum_{j=1}^{p} w_j^2
$$

* $\alpha$ (또는 $\lambda$): 규제의 강도를 조절하는 하이퍼파라미터이다. $\alpha$가 클수록 규제가 강해져 가중치 값이 더 작아진다.

릿지 회귀는 


## 4. 라쏘 회귀 (Lasso Regression) - L1 규제

라쏘 회귀는 RSS에 **가중치들의 절대값 합**에 비례하는 페널티 항을 추가한다.

### 목적 함수
$$
J(w) = RSS(w) + \alpha \sum_{j=1}^{p} |w_j|
$$

### 특징
*   중요도가 낮은 변수의 가중치를 **완전히 0**으로 만든다.
*   이를 통해 모델에서 불필요한 변수를 제거하는 **변수 선택(Feature Selection)** 효과를 얻을 수 있다.


## 5. Ridge Regression on 2-Stage Stacking

다양한 모형을 만든 뒤, Stacking을 통해 보다 강력한 예측 모형을 만들 수 있다. 이때 2-Stage에서 사용하는 메타 모델은 비교적 단순하고, 선형 회귀보다는 다중공선성에 강한 Ridge Regression을 주로 사용한다.