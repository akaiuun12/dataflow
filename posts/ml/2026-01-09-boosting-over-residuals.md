---
title: Boosting Over Residuals for Linear Data
date: 2026-01-09
author: Akai Red
image: /imgs/2026-01-09-boosting-over-residuals-01.png
tags: ['machine learning', 'boosting over residuals', 'tabular data', 'regression', 'xgboost']
published: true
---

ë°ì´í„°ê°€ ê°•í•œ ì„ í˜•ì„±ì„ ì§€ë‹ˆê³  ìˆì„ ë•Œ, ìºê¸€(Kaggle) ë“± ë¨¸ì‹ ëŸ¬ë‹ ê²½ì§„ëŒ€íšŒì—ì„œ ìƒìœ„ê¶Œ ìŠ¤ì½”ì–´ë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•´ ì‚¬ìš©í•˜ëŠ” ê¸°ë²• ì¤‘ í•˜ë‚˜ê°€ ë°”ë¡œ **Boosting Over Residuals**ì´ë‹¤.

Boosting Over ResidualsëŠ” ì„ í˜• ëª¨í˜•ì— ë°ì´í„°ë¥¼ ì í•©í•œ ë’¤, ì”ì°¨ì— ëŒ€í•´ì„œ XGBoost ë“± ë³µì¡í•œ ë¹„ì„ í˜• ëª¨í˜•ì„ í•™ìŠµí•˜ëŠ” ê²ƒì´ë‹¤. ë°ì´í„°ì˜ ê°•í•œ ì„ í˜•ì„±ì€ ì„ í˜• ëª¨ë¸ë¡œ ì¡ë˜, ì„ í˜• ëª¨í˜•ì´ ì¡ì§€ ëª»í•˜ëŠ” ì˜¤ë¥˜(ì”ì°¨)ëŠ” ê°•ë ¥í•œ ë¹„ì„ í˜• ëª¨í˜•ìœ¼ë¡œ ì¡ê² ë‹¤ëŠ” ê²ƒì´ë‹¤.

ì¦‰, ë”¥ëŸ¬ë‹ì—ì„œ ë§í•˜ëŠ” ì”ì°¨ í•™ìŠµ(Residual Learning)ê³¼ ì™„ì „íˆ ë™ì¼í•œ ê°œë…ì´ë‹¤. ê·¸ëŸ¬ë‚˜ ì”ì°¨ í•™ìŠµì´ë¼ê³  ì“°ë©´ ëª¨ë‘ ë”¥ëŸ¬ë‹ê³¼ í—·ê°ˆë¦´ ê²ƒ ê°™ì•„ì„œ ì´ í¬ìŠ¤íŒ…ì—ì„œëŠ” Boosting Over Residualsë¼ê³  ë¶€ë¥´ë„ë¡ í•˜ê² ë‹¤. ìš©ì–´ëŠ” í•´ë‹¹ ê¸°ë²•ì„ ì •í˜• ë°ì´í„°ì— ì ìš©í•œ ìºê¸€ í¬ìŠ¤íŠ¸ë¥¼ ì°¸ê³ í•˜ì˜€ë‹¤.


## Boosting Over Residualsì˜ ë‹¨ê³„
Boosting Over Residualsì˜ ë…¼ë¦¬ëŠ” ë§¤ìš° ëª…ì¾Œí•˜ë‹¤.

1.  **ì„ í˜• ëª¨í˜• ì í•©**: ë¨¼ì € ë‹¨ìˆœí•œ ì„ í˜• íšŒê·€(Linear Regression)ë‚˜ Ridge, Lasso ëª¨í˜•ì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ ì í•©ì‹œí‚¨ë‹¤. ì´ ê³¼ì •ì—ì„œ ë°ì´í„°ì˜ êµµì§í•œ ì„ í˜•ì  íë¦„ì„ ì¡ì•„ë‚¸ë‹¤.
2.  **ì”ì°¨(Residual) ê³„ì‚°**: ì‹¤ì œ íƒ€ê²Ÿ ê°’($y$)ì—ì„œ ì„ í˜• ëª¨í˜•ì˜ ì˜ˆì¸¡ê°’($\hat{y}_{linear}$)ì„ ëº€ ì”ì°¨($r = y - \hat{y}_{linear}$)ë¥¼ êµ¬í•œë‹¤. ì´ ì”ì°¨ëŠ” ì„ í˜• ëª¨í˜•ì´ ë¯¸ì²˜ ì¡ì•„ë‚´ì§€ ëª»í•œ 'ë³µì¡í•œ íŒ¨í„´'ì´ë‚˜ 'ì˜¤ë¥˜'ë¥¼ ë‹´ê³  ìˆë‹¤.
3.  **ë¹„ì„ í˜• ëª¨í˜• í•™ìŠµ**: **ì”ì°¨(r)ë¥¼ ìƒˆë¡œìš´ íƒ€ê²Ÿ**ìœ¼ë¡œ í•˜ì—¬ XGBoostë‚˜ Neural Network ê°™ì€ ê°•ë ¥í•œ ë¹„ì„ í˜• ëª¨í˜•ì„ í•™ìŠµì‹œí‚¨ë‹¤.
4.  **ìµœì¢… ì˜ˆì¸¡**: ìµœì¢… ì˜ˆì¸¡ê°’ì€ ë‘ ëª¨í˜•ì˜ ì˜ˆì¸¡ê°’ì„ ë”í•œ ê°’ì´ ëœë‹¤.
    $$\hat{y}_{final} = \hat{y}_{linear} + \hat{y}_{residual\_nonlinear}$$

ì´ë ‡ê²Œ í•˜ë©´ ì„ í˜• ëª¨ë¸ì´ í° í‹€ì—ì„œì˜ ê²½í–¥ì„±ì„ ì±…ì„ì§€ê³ , ë¹„ì„ í˜• ëª¨ë¸ì€ ê·¸ ì„¸ë¶€ì ì¸ ë””í…Œì¼(ì”ì°¨)ë§Œ ë³´ì •í•˜ëŠ” ì—­í• ì„ ìˆ˜í–‰í•˜ê²Œ ë˜ì–´ ì „ì²´ì ì¸ ì„±ëŠ¥ì´ í–¥ìƒë˜ëŠ” ê²½ìš°ê°€ ë§ë‹¤.

![](imgs/2024-07-16-ensemble-comparison.png)


## Boosting Over Residualsì˜ êµ¬í˜„
`scikit-learn`ì˜ `LinearRegression`ê³¼ `xgboost`ì˜ `XGBRegressor`ë¥¼ ì‚¬ìš©í•˜ì—¬ ê°„ë‹¨í•˜ê²Œ êµ¬í˜„í•  ìˆ˜ ìˆë‹¤. 

ë°ì´í„°ë¥¼ ë¨¼ì € `LinearRegression`ë¡œ ì í•©ì‹œí‚¤ê³ , `residuals`ë¥¼ ê³„ì‚°í•œ ë’¤, `residuals`ë¥¼ íƒ€ê²Ÿìœ¼ë¡œ ë¹„ì„ í˜• ëª¨í˜•ì„ í•™ìŠµì‹œí‚¨ë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ ì„ í˜• ëª¨í˜•ì˜ ì˜ˆì¸¡ê°’`test_linear_pred$`ê³¼ ë¹„ì„ í˜• ëª¨í˜•ì˜ ì˜ˆì¸¡ê°’`test_residual_pred`ì„ ë”í•˜ì—¬ ìµœì¢… ì˜ˆì¸¡ê°’ì„ ì–»ëŠ”ë‹¤.

```python
import numpy as np
from sklearn.linear_model import LinearRegression
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error

# 1. ì„ í˜• ëª¨ë¸ í•™ìŠµ
lr_model = LinearRegression()
lr_model.fit(X_train, y_train)

# 2. ì„ í˜• ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’ ë° ì”ì°¨ ê³„ì‚°
train_linear_pred = lr_model.predict(X_train)
residuals = y_train - train_linear_pred

# 3. ì”ì°¨ë¥¼ íƒ€ê²Ÿìœ¼ë¡œ ë¹„ì„ í˜• ëª¨ë¸(XGBoost) í•™ìŠµ
xgb_model = XGBRegressor(n_estimators=100, learning_rate=0.05)
xgb_model.fit(X_train, residuals)

# 4. ìµœì¢… ì˜ˆì¸¡ (ì„ í˜• ì˜ˆì¸¡ + ì”ì°¨ ì˜ˆì¸¡)
test_linear_pred = lr_model.predict(X_test)
test_residual_pred = xgb_model.predict(X_test)

final_pred = test_linear_pred + test_residual_pred

print(f"RMSE: {np.sqrt(mean_squared_error(y_test, final_pred))}")
```


## Boosting Over Residualsì˜ íš¨ê³¼ ë° ì£¼ì˜í•  ì 

### ì•ˆì •ì ì¸ ë² ì´ìŠ¤ë¼ì¸
ì„ í˜• ëª¨ë¸ì´ ì´ë¯¸ ë°ì´í„°ì˜ ìƒë‹¹ ë¶€ë¶„ì„ ì„¤ëª…í•´ì£¼ê¸° ë•Œë¬¸ì—, ë¹„ì„ í˜• ëª¨ë¸ì´ í•™ìŠµí•´ì•¼ í•  ëŒ€ìƒ(ì”ì°¨)ì˜ ë¶„ì‚°ì´ ì‘ì•„ì ¸ í•™ìŠµì´ ë” ì•ˆì •ì ìœ¼ë¡œ ì§„í–‰ë  ìˆ˜ ìˆë‹¤.

### ì•™ìƒë¸” íš¨ê³¼
ì„œë¡œ ë‹¤ë¥¸ ë©”ì»¤ë‹ˆì¦˜ì„ ê°€ì§„ ë‘ ëª¨ë¸ì„ ê²°í•©í•˜ëŠ” í˜•íƒœì´ë¯€ë¡œ, ë‹¨ì¼ ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒë³´ë‹¤ ì¼ë°˜í™” ì„±ëŠ¥ì´ ì¢‹ì•„ì§€ëŠ” ê²½í–¥ì´ ìˆë‹¤.

### ì£¼ì˜í•  ì 
ëª¨ë“  ë°ì´í„°ì—ì„œ íš¨ê³¼ì ì¸ ê²ƒì€ ì•„ë‹ˆë©° ë°ì´í„°ì— ì„ í˜•ì„±ì´ ë¶€ì¡±í•˜ë‹¤ë©´ ì˜¤íˆë ¤ ë¶ˆí•„ìš”í•œ ë³µì¡ë„ë§Œ ì¦ê°€ì‹œí‚¬ ìˆ˜ ìˆë‹¤. ë˜í•œ ì„ í˜• ëª¨í˜•ê³¼ ë¹„ì„ í˜• ëª¨í˜•ì„ ê°ê° í•™ìŠµí•  ë•Œ ë°ì´í„° ëˆ„ìˆ˜(Data Leakage)ê°€ ë°œìƒí•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì£¼ì˜í•´ì•¼ í•œë‹¤.

![í˜„ì¬ê¹Œì§€ëŠ” Boosting Over Residualsê°€ ë‚´ Submission ì¤‘ ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì¤¬ë‹¤.](../../images/986f135ebd0944e30982f54a4683afce1944df7353c8f330110793486a672fdf.png)  


## References
- [XGB Boosting Over Residuals - CV 0.05595 ğŸ‰ ğŸ¥³ ğŸ¥‡](https://www.kaggle.com/competitions/playground-series-s5e10/discussion/610828)