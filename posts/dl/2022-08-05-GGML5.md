---
layout: post
category: DL
tags: ['deep learning']
date: 2022-08-05

title: GGML Coursera - C1W3 Shallow Neural Network 
author: Akai Red
description:
    지난 주에 로지스틱 회귀를 하는 방법을 배웠다. 로지스틱 회귀는 입력이 하나의 노드를 거쳐 결과를 산출한다. 그렇다면 신경망은 무엇이 다른가? 신경망은 노드를 쌓아서 하나의 층을 만든다.

image: https://res.cloudinary.com/dhchweuhy/image/upload/c_fill,w_760,h_399,r_5,f_auto,q_auto/lrc/20220805_img2
optimized_image: https://res.cloudinary.com/dhchweuhy/image/upload/c_fill,w_380,h_200,r_5,f_auto,q_auto/lrc/20220805_img2
                    
show_thumbnail: true
math: true
published: True
--- 

## Neural Networks Overview

![](https://res.cloudinary.com/dhchweuhy/image/upload/c_fill,w_760,h_399,r_5,f_auto,q_auto/lrc/20220805_img1)

지난 주에 로지스틱 회귀를 하는 방법을 배웠다. 로지스틱 회귀를 계산 그래프로 그리면 왼쪽 위와 같이 된다. 입력이 하나의 노드를 거쳐 결과를 산출한다. 그렇다면 신경망은 무엇이 다른가? 신경망은 노드를 쌓아서 하나의 층을 만든다. 강의에선 일단 로지스틱 회귀를 두 단계 '이어서' 진행하는 것으로 신경망을 구현한다.


## Neural Network Represenatation 

![](https://res.cloudinary.com/dhchweuhy/image/upload/c_fill,w_760,h_399,r_5,f_auto,q_auto/lrc/20220805_img2)

노드의 층을 레이어라고 한다. 시작할 때의 입력값은 **입력층(Input Layer)**, 결과값은 **출력층(Output Layer)**라고 한다. 그리고 그 사이에 선형결합이나 로지스틱 변환 등을 담당했던 노드가 모여 만든 층이 있다. 이 층을 **은닉층(Hidden Layer)**이라고 부른다. 역전파 과정을 통해 은닉층의 가중치를 '학습'시키는 것이 신경망의 핵심이다. 보통은 입력층은 세지 않기 때문에 위의 구조는 2층 신경망이 된다.


## Computing a Neural Network's Output

![](https://res.cloudinary.com/dhchweuhy/image/upload/c_fill,w_760,h_399,r_5,f_auto,q_auto/lrc/20220805_img3)

신경망 노드의 계산 원리 자체는 로지스틱 회귀 때와 동일하다. 입력값의 선형 결합을 시그모이드 함수를 통해 변환한다. 다만 노드가 하나였던 로지스틱 회귀와 달리 여러 노드가 하나의 층을 이루고 있기 때문에 각 층의 모든 노드에 대해 동일한 방식으로 계산을 해줘야 한다. 

신경망에 사용하는 표기법도 등장한다. $a_{i}^{[l]}$는 $l$번째 층의 $i$번째 노드를 지칭한다. 입력층은 $a^{[0]}$로 표기하곤 한다. 

![](https://res.cloudinary.com/dhchweuhy/image/upload/c_fill,w_760,h_399,r_5,f_auto,q_auto/lrc/20220805_img4)

각 노드마다 계산을 해줘야하므로 벡터 연산이 효과적이다. 각 노드별 가중치를 행으로 갖는 행렬 $W$를 만들면 나머지는 간단한 행렬 계산으로 구할 수 있다. 물론 헷갈리기 때문에 각 차원 크기를 잘 따져가면서 이해하자. 식의 파란색 부분이 아래 행렬 계산의 파란색 부분과 일치하는 것을 알 수 있다.

처음에 가중치 벡터 $w$의 존재가 참 이해하기 어려웠는데, 신경망에서 화살표라고 생각하니 바로 이해가 됐다. 행렬은 선형 매핑이기 때문에 $W$는 행렬인 것이다.


## Vectorzing Across Multiple Examples

![](https://res.cloudinary.com/dhchweuhy/image/upload/c_fill,w_760,h_399,r_5,f_auto,q_auto/lrc/20220805_img5)

방금까지의 과정이 하나의 x값에 대한 계산 과정이다. 다들 잘 알고 있겠지만 머신러닝에는 빅데이터가 사용되고 이러한 연산을 각 데이터마다 해줘야한다. for문을 사용할 수도 있지만 데이터의 크기가 워낙 큰 ML 분야에서 for문의 사용은 가급적 권장하지 않는다. 대신 벡터화해서 빠른 계산을 노린다. 아래에서 더 자세히 설명한다.


## Explanation for Vectorized Implementation

![](https://res.cloudinary.com/dhchweuhy/image/upload/c_fill,w_760,h_399,r_5,f_auto,q_auto/lrc/20220805_img6)

$z^{[1](1)}$ 는 두번째 데이터의 레이어 1 출력값을 의미하는 표기법이다. 색상별로 잘 칠해져있으니 행렬 곱으로 각 레이어의 출력값 행렬을 구할 수 있음을 알 수 있다. 행렬 $Z^{[1]}$은 전체 샘플에 대한 레이어 1의 출력값을 의미하며 각 열은 각 샘플별 레이어 1 출력값 $z^{[1](i)}$를 나타낸다.


## Activation Functions
![](https://res.cloudinary.com/dhchweuhy/image/upload/c_fill,w_760,h_399,r_5,f_auto,q_auto/lrc/20220805_img7)

각 층의 노드는 선형 결합만을 수행하진 않는다. '선형결합 + 활성화'를 묶어서 하나의 노드를 형성한다. 로지스틱 회귀 예시에선 활성화 함수가 로지스틱 함수였다. 그러나 신경망 모형의 함수가 반드시 로지스틱 함수여야하는 것은 아니다. 오른쪽은 시그모이드 대신 사용하는 tanh 함수의 그래프와 식이 적혀있다.

![](https://res.cloudinary.com/dhchweuhy/image/upload/c_fill,w_760,h_399,r_5,f_auto,q_auto/lrc/20220805_img8)

여기서는 가장 대표적으로 사용하는 활성화 함수를 소개한다. 시그모이드는 로지스틱 회귀에서 사용한 활성화 함수지만 은닉층에서는 잘 사용하지 않는다고 한다. 왜냐하면 tanh 함수가 시그모이드보다 거의 모든 경우에 더 나은 퍼포먼스를 보이기 때문이다. 

또 한가지 많이 사용하는 것은 ReLU 함수다. ReLU 함수는 0보다 큰 값에 대해 x를 반환하기 때문에 더 빠른 역전파가 일어난다. ReLU는 수학적으로는 $x=0$에서 미분불가능이다. 역전파에 치명적인 문제일 것 같지만 실제로는 정확히 0이 값으로 나오는 경우는 거의 없기 때문에 실무에선 큰 문제가 되지 않는다고. 0보다 작은 값에서 미분이 0이 되는 것이 문제가 될 경우에는 Leaky ReLU를 사용한다.


## Why do you need Non-Linear Activation Functions?

![](https://res.cloudinary.com/dhchweuhy/image/upload/c_fill,w_760,h_399,r_5,f_auto,q_auto/lrc/20220805_img9)

활성화 함수에는 여러 종류가 있지만 중요한 것은 모두 비선형 함수여야한다는 것이다. 왜 그럴까? 선형 함수의 특성은 $f(ax +b) = af(x) + b$라는 것이다. 이러한 특성 때문에 선형 함수끼리의 합성은 모두 선형함수다. 결국 굳이 활성화 함수를 지정하는 의미도, 여러 층을 쌓는 의미도 없어지게 된다.


## Derivatives of Activation Functions

![](https://res.cloudinary.com/dhchweuhy/image/upload/c_fill,w_760,h_399,r_5,f_auto,q_auto/lrc/20220805_img10)

활성화 함수로 다른 함수를 사용하기로 했다면 역전파를 위해 미분값을 구해야한다. $tanh(z)$의 미분값은 위에 나와있는 것처럼 $1 - {tanh(z)}^2$으로 구할 수 있다. ReLU나 Leaky ReLU는 더욱 간단한데 0보다 클 경우 미분값은 1, 0보다 작을 경우는 각각 0, 0.01이 된다.


## Gradient Descent for Neural Networks

![](https://res.cloudinary.com/dhchweuhy/image/upload/c_fill,w_760,h_399,r_5,f_auto,q_auto/lrc/20220805_img11)
![](https://res.cloudinary.com/dhchweuhy/image/upload/c_fill,w_760,h_399,r_5,f_auto,q_auto/lrc/20220805_img12)

경사하강법과 역전파 과정은 계산은 복잡하지만 개념적으로는 로지스틱 회귀와 다를 것이 없다. 미분량을 구한 뒤 기존의 파라미터에서 학습률 * 미분량 만큼을 빼가는 것을 반복한다. 연쇄법칙을 하나씩 적용해가면 원하는 결과를 얻을 수 있다. 


## Random Initialization

![](https://res.cloudinary.com/dhchweuhy/image/upload/c_fill,w_760,h_399,r_5,f_auto,q_auto/lrc/20220805_img13)

활성화 함수로 선형 함수를 쓰면 안되는 것처럼 가중치 초기화에도 중요한 황금률이 있다. 그건 바로 모든 가중치의 초기값을 0으로 하면 안된다는 것이다. 모든 가중치가 0이면 모든 노드의 계산값이 같아진다. 당연히 역전파가 제대로 일어나지 않고 학습이 이루어지지 않는다. 이 때문에 가중치는 아주 작지만 0이 아닌 랜덤값을 많이 사용한다.

+ 왜 가중치로 큰 값이 아닌 작은 값을 사용할까?  
활성화 함수가 시그모이드나 tanh인 경우, 큰 값이 들어가면 활성화 함수의 그래프에서 거의 평탄한 구간에 들어가게 된다. 이 구간에서는 미분값이 거의 0이기 때문에 역전파가 제대로 일어나지 않는다. 이를 **기울기 소실(Vanishing Gradient)** 문제라고 한다. 활성화 함수로 ReLU를 사용하면 이러한 문제를 어느 정도 해결해준다. 기울기 소실에 관한 얘기는 나중에 더 자세히 다룬다.


## C​opyright Notice

나는 슬라이드에서 명시한 저작권 기준 CCL를 최대한 따르고자 했고 Coursera 강의와 관련한 앞으로의 포스트도 그러할 것이다. [DeepLearning.AI](https://www.deeplearning.ai/)은 첨부한 슬라이드 이미지의 원 저작자임이며 본 글은 영리적 목적이 아닌 교육적 목적으로 작성되었음을 밝힌다. 또한 슬라이드 이미지의 저작권은 원 저작자의 뜻대로 [Creative Commons 2.0](https://creativecommons.org/licenses/by-sa/2.0/legalcode)을 따른다.