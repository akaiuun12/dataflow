---
title: AI에게 배우는 Kaggle Competition
date: 2025-12-25
author: Akai Red
image: 
published: true
---

Kaggle은 다양한 Competition을 통해 데이터 분석 능력을 향상시킬 수 있는 플랫폼이다. 


## Kaggle Competition
```python
import os
import numpy as np
import pandas as pd
```

```python
TRAIN_PATH = 'data/train.csv'
TEST_PATH = 'data/test.csv'
SAMPLE_SUBMISSION_PATH = 'data/sample_submission.csv'

train = pd.read_csv(TRAIN_PATH)
test = pd.read_csv(TEST_PATH)
sample_submission = pd.read_csv(SAMPLE_SUBMISSION_PATH)
```

## GPU
```python
import cudf
import cuml
import cupy as cp
```

## Google Gemini 
```python
# Kaggle x Gemini 전략 프롬프트 모음
import os

import numpy as np
import pandas as pd

# GPU acceleration
import cuml
import cudf
import cupy

# DL Framework
import tensorflow as tf

# VARIABLES
TRAIN_PATH = '/kaggle/input/playground-series-s5e12/train.csv'
TEST_PATH = '/kaggle/input/playground-series-s5e12/test.csv'
SAMPLE_SUBMISSION_PATH = '/kaggle/input/playground-series-s5e12/sample_submission.csv'
TARGET = 'diagnosed_diabetes'
N_FOLDS = 5

OBJECTIVE = 'ROC-AUC'
MODEL_CANDIDATES = 'XGBoost, RandomForest, LightGBM, CatBoost, Wide&Deep'

# Load Data
X_train = pd.read_csv(TRAIN_PATH)
y_train = X_train[TARGET]
X_train = X_train.drop(TARGET, axis=1)
X_test = pd.read_csv(TEST_PATH)

gemini_prompts = f'''
나는 현재 Kaggle Competition에 참여 중이야. 

- 훈련 데이터셋 
    - 변수 목록: {X_train.columns}
    - 변수 타입: {X_train.dtypes}
    - 변수 결측치 개수: {X_train.isnull().sum()}
    - 변수 중복 개수: {X_train.duplicated().sum()}
    - 변수 유일 개수: {X_train.nunique()}
    - 타겟 분포: {y_train.value_counts()}

- Python 버전: {os.version},
- CPU/GPU 환경: {tf.config.list_physical_devices('GPU')},

- 패키지 목록: 
    Numpy Version: {np.__version__},
    Pandas Version: {pd.__version__},
    cudf Version: {cudf.__version__}, 
    cuml Version: {cuml.__version__}, 
    cupy Version: {cupy.__version__}, 
    Tensorflow Version: {tf.__version__}

- 훈련 데이터 경로: {TRAIN_PATH},
- 검사 데이터 경로: {TEST_PATH},
- 샘플 제출 파일 경로: {SAMPLE_SUBMISSION_PATH}

- 타겟 변수: {TARGET}
- 목적 함수: {OBJECTIVE}

1. 데이터셋의 특성을 고려하여 적절한 Feature Engineering을 수행해줘.
2. 결측치 처리와 이상치 제거를 위해 효율적인 전처리 파이프라인을 만들어줘.
3. 클래스 불균형을 해결하기 위한 방법을 적용해줘.
4. 필요 시 스케일링 및 라벨 인코딩을 적용해줘.
5. {MODEL_CANDIDATES} 중 가장 적합한 모델 3개를 선택해줘.
6. 과적합을 방지하기 위한 Stratified {N_FOLDS}-Fold 교차 검증을 수행해줘.
7. 가장 적합한 3가지 모델을 조합하여 앙상블 모델을 만들어줘.
8. 검사 데이터를 예측하여 제출 파일을 생성해줘.
'''
```


```python
# Kaggle x Gemini Prompts (JSON format)
import os

import numpy as np
import pandas as pd

# GPU acceleration
import cuml
import cudf
import cupy

# DL Framework
import tensorflow as tf

# VARIABLES
TRAIN_PATH = '/kaggle/input/playground-series-s5e12/train.csv'
TEST_PATH = '/kaggle/input/playground-series-s5e12/test.csv'
SAMPLE_SUBMISSION_PATH = '/kaggle/input/playground-series-s5e12/sample_submission.csv'

TARGET = 'diagnosed_diabetes'
EVALUATION_METRIC = 'ROC-AUC'
MODEL_CANDIDATES = 'XGBoost, RandomForest, LightGBM, CatBoost, Wide&Deep'

N_FOLDS = 5

# Load Data
X_train = pd.read_csv(TRAIN_PATH)
y_train = X_train[TARGET]
X_train = X_train.drop(TARGET, axis=1)
X_test = pd.read_csv(TEST_PATH)


# JSON 형식으로 전략 프롬프트 생성
gemini_prompts_json = {
  "competition_context": {
    "target_variable": TARGET,
    "evaluation_metric": EVALUATION_METRIC,
  },
  "environment_info": {
    "python_version": sys.version,
    "hardware": {
      "gpu": tf.config.list_physical_devices('GPU'),
      "cpu": tf.config.list_physical_devices('CPU'),
    },
    "package_versions": {
      "numpy": np.__version__,
      "pandas": pd.__version__,
      "cudf": cudf.__version__,
      "cuml": cuml.__version__,
      "cupy": cupy.__version__,
      "tensorflow": tf.__version__
    }
  },
  "dataset_summary": {
    "file_paths": {
      "train": TRAIN_PATH,
      "test": TEST_PATH,
      "sample_submission": SAMPLE_SUBMISSION_PATH
    },
    "data_stats": {
      "total_rows": X_train.shape[0],
      "missing_values": X_train.isnull().sum().sum(),
      "duplicates": X_train.duplicated().sum(),
      "target_distribution": y_train.value_counts().to_dict()
    },
    "feature_metadata": X_train.columns.tolist()
  },
  "modeling_requirements": {
    "model_candidates": MODEL_CANDIDATES,
    "validation_strategy": f"Stratified {N_FOLDS}-Fold Cross Validation",
    "requested_tasks": [
      {
        "step": 1,
        "task": "Preprocessing Pipeline",
        "description": "Null handling and outlier removal"
      },
      {
        "step": 2,
        "task": "Feature Engineering",
        "description": "Data features를 고려한 파생 변수 생성"
      },
      {
        "step": 3,
        "task": "Class Imbalance Handling",
        "description": "Target distribution imbalance handling strategy"
      },
      {
        "step": 4,
        "task": "Data Transformation",
        "description": "Scaling and label encoding"
      },
      {
        "step": 5,
        "task": "Model Selection",
        "description": "Best 3 models from candidates"
      },
      {
        "step": 6,
        "task": "Cross Validation",
        "description": "Stratified {N_FOLDS}-Fold Cross Validation to prevent overfitting"
      },
      {
        "step": 7,
        "task": "Ensemble",
        "description": "Ensemble of selected 3 models (Blending/Stacking)"
      },
      {
        "step": 8,
        "task": "Submission",
        "description": "Test data prediction and submission file creation"
      }
    ]
  }
}

print(gemini_prompts_json)
```

## Codes
### 1. 라이브러리 로드 및 환경 설정
```python
import pandas as pd
import numpy as np
import cudf  # NVIDIA GPU를 사용하여 Pandas 연산을 가속화합니다.
import xgboost as xgb
import lightgbm as lgb
from catboost import CatBoostClassifier
from sklearn.ensemble import HistGradientBoostingClassifier
from sklearn.model_selection import StratifiedKFold, KFold
from sklearn.metrics import roc_auc_score
from sklearn.preprocessing import LabelEncoder
from scipy.stats import rankdata, hmean
import warnings

warnings.filterwarnings('ignore') # 분석에 방해되는 경고 문구를 숨깁니다.
```

### 2. 데이터 로딩 (Data Loading)
```python
# ==========================================
# 1. DATA LOADING
# ==========================================
print(">>> Loading Data...")
train_df = cudf.read_csv('/kaggle/input/playground-series-s5e12/train.csv').to_pandas()
test_df = cudf.read_csv('/kaggle/input/playground-series-s5e12/test.csv').to_pandas()
```

### 3. 피처 엔지니어링 (Feature Engineering)
```python
# ==========================================
# 2. FEATURE ENGINEERING (The 0.70+ Breakthrough)
# ==========================================
def engineer_features(df):
    # Fix impossible zeros
    cols_to_fix = ['bmi', 'systolic_bp', 'diastolic_bp', 'cholesterol_total']
    for col in cols_to_fix:
        df[col] = df[col].replace(0, np.nan)
    
    # 1. CLINICAL BINS (The "Hard Threshold" Tweak)
    # BMI Categories
    df['bmi_cat'] = pd.cut(df['bmi'], bins=[0, 18.5, 25, 30, 100], labels=[0, 1, 2, 3]).astype(float)
    # Blood Pressure Categories (JNC 7 Criteria)
    df['bp_status'] = np.where(df['systolic_bp'] < 120, 0, 
                               np.where(df['systolic_bp'] < 140, 1, 2))
    
    # 2. RATIOS & METABOLIC LOAD
    df['map'] = df['diastolic_bp'] + (df['systolic_bp'] - df['diastolic_bp']) / 3
    df['chol_hdl_ratio'] = df['cholesterol_total'] / (df['hdl_cholesterol'] + 1e-6)
    
    # 3. AGE-WEIGHT INTERACTION (Age is the biggest risk factor)
    df['age_risk_index'] = (df['age'] * df['bmi']) / 100
    df['years_of_high_bp'] = (df['age'] - 30).clip(0) * df['bp_status']
    
    # 4. SEDENTARY LOAD
    df['net_activity'] = df['physical_activity_minutes_per_week'] - (df['screen_time_hours_per_day'] * 60)
    
    return df

print(">>> Engineering Metabolic Features...")
train_df = engineer_features(train_df)
test_df = engineer_features(test_df)```
```

### 4. 범주형 데이터 인코딩 (Encoding)
# ==========================================
# 3. TARGET ENCODING
# ==========================================
```python
cat_cols = ['ethnicity', 'education_level', 'employment_status', 'income_level', 'smoking_status']
binary_cols = ['gender', 'family_history_diabetes', 'hypertension_history', 'cardiovascular_history']

def apply_target_encoding(train, test, columns):
    kf = KFold(n_splits=5, shuffle=True, random_state=42)
    for col in columns:
        new_col = f"{col}_te"
        train[new_col] = 0.0
        for tr_idx, val_idx in kf.split(train):
            means = train.iloc[tr_idx].groupby(col)['diagnosed_diabetes'].mean()
            train.loc[val_idx, new_col] = train.iloc[val_idx][col].map(means)
        test[new_col] = test[col].map(train.groupby(col)['diagnosed_diabetes'].mean())
        train[new_col].fillna(train['diagnosed_diabetes'].mean(), inplace=True)
        test[new_col].fillna(train['diagnosed_diabetes'].mean(), inplace=True)
    return train.drop(columns=columns), test.drop(columns=columns)

train_df, test_df = apply_target_encoding(train_df, test_df, cat_cols)

for col in binary_cols:
    le = LabelEncoder()
    train_df[col] = le.fit_transform(train_df[col].astype(str))
    test_df[col] = le.transform(test_df[col].astype(str))

X = train_df.drop(['id', 'diagnosed_diabetes'], axis=1)
y = train_df['diagnosed_diabetes']
X_test = test_df.drop(['id'], axis=1)
```

### 5. 모델 정의 및 학습 전략 (Model Diversity)
```python
# ==========================================
# 4. TRAINING WITH 4-MODEL DIVERSITY
# ==========================================
def get_models():
    return [
        ('xgb', xgb.XGBClassifier(booster='dart', rate_drop=0.1, n_estimators=1000, learning_rate=0.05, 
                                 max_depth=6, tree_method='hist', device='cuda', random_state=42)),
        ('lgb', lgb.LGBMClassifier(n_estimators=1500, learning_rate=0.03, device='gpu', random_state=42, verbose=-1)),
        ('cat', CatBoostClassifier(iterations=1500, learning_rate=0.03, depth=6, task_type='GPU', verbose=False, random_state=42)),
        ('hgb', HistGradientBoostingClassifier(max_iter=500, learning_rate=0.05, max_depth=8, random_state=42))
    ]

skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
oof_preds = np.zeros((len(X), 4))
test_preds = np.zeros((len(X_test), 4))

for fold, (tr_idx, val_idx) in enumerate(skf.split(X, y)):
    print(f"--- FOLD {fold+1} ---")
    Xt, Xv = X.iloc[tr_idx], X.iloc[val_idx]
    yt, yv = y.iloc[tr_idx], y.iloc[val_idx]
    
    for i, (name, model) in enumerate(get_models()):
        model.fit(Xt, yt)
        p = model.predict_proba(Xv)[:, 1]
        oof_preds[val_idx, i] = p
        test_preds[:, i] += model.predict_proba(X_test)[:, 1] / 5
        print(f"Fold {fold+1} {name} AUC: {roc_auc_score(yv, p):.5f}")
```

### 6. 교차 검증 및 학습 실행 (Training Loop)
```python
# ==========================================
# 5. ENSEMBLE (Harmonic Mean of Ranks)
# ==========================================
# We convert to ranks, then take the Harmonic Mean. 
# Harmonic mean is less sensitive to outliers than Arithmetic mean.
print(">>> Blending with Rank-based Harmonic Mean...")
final_ranks = np.zeros(test_preds.shape)
for i in range(4):
    final_ranks[:, i] = rankdata(test_preds[:, i])

# Add a small epsilon to avoid division by zero in hmean
final_probs = hmean(final_ranks + 1, axis=1)
```

### 7. 앙상블 및 최종 제출 (Rank-based Blending)
```python
# ==========================================
# 6. SUBMISSION
# ==========================================
sub = pd.read_csv('/kaggle/input/playground-series-s5e12/sample_submission.csv')
sub['diagnosed_diabetes'] = final_probs
sub.to_csv('submission.csv', index=False)
print("Final Optimized Submission Saved!")
```